# RAG Pipeline Modular Configuration
# Streamworks-KI - Professional RAG System Configuration

# Pipeline Configuration
pipeline:
  name: "streamworks_rag_modular"
  version: "2.0.0"
  mode: "development"  # development|production
  
# Reranker Configuration
reranker:
  enabled: true
  primary_provider: "local"          # local|cohere|none
  fallback_providers: ["cohere"]     # Fallback providers in order
  auto_fallback: true                # Automatically fallback on failure
  health_check_enabled: true         # Health check before usage
  
  providers:
    local:
      model: "cross-encoder/ms-marco-MiniLM-L-12-v2"  # High-quality cross-encoder
      device: "auto"                 # auto|cpu|mps|cuda
      max_length: 512               # Input sequence length
      batch_size: 32                # Batch size for inference
      top_k: 5                      # Number of results to return
      cache_dir: "./models/rerankers"
      
      # Alternative models for different use cases
      alternative_models:
        - "cross-encoder/ms-marco-MiniLM-L-6-v2"    # Faster
        - "cross-encoder/ms-marco-TinyBERT-L-2-v2"  # Fastest
        - "cross-encoder/ms-marco-electra-base"      # Higher quality
    
    cohere:
      model: "rerank-english-v2.0"   # Cohere rerank model
      api_key_env: "RERANKER_API_KEY"
      max_retries: 3
      timeout: 30
      top_k: 5
      
      alternative_models:
        - "rerank-multilingual-v2.0"  # Multilingual support
        - "rerank-english-v3.0"       # Newer version

# Embedding Configuration  
embeddings:
  primary_provider: "openai"         # openai|local
  fallback_provider: "local"         # Fallback if primary fails
  auto_fallback: true
  
  providers:
    openai:
      model: "text-embedding-3-large"
      api_key_env: "OPENAI_API_KEY"
      dimensions: 3072
      batch_size: 100
      max_retries: 3
      timeout: 30
    
    local:
      model: "sentence-transformers/all-MiniLM-L6-v2"
      dimensions: 384
      device: "auto"                 # auto|cpu|mps|cuda
      batch_size: 64
      max_length: 512
      normalize_embeddings: true
      
      # Alternative embedding models
      alternative_models:
        - "sentence-transformers/all-mpnet-base-v2"      # Higher quality
        - "sentence-transformers/paraphrase-MiniLM-L6-v2" # Good for similarity
        - "sentence-transformers/distilbert-base-nli-mean-tokens"  # Fast

# LLM Configuration
llm:
  primary_provider: "ollama"         # ollama|openai
  fallback_provider: "openai"       # Fallback if primary fails  
  auto_fallback: true
  
  providers:
    ollama:
      base_url: "http://localhost:11434"
      model: "qwen2.5:7b"           # Primary model for RAG
      temperature: 0.1
      max_tokens: 2000
      timeout: 60
      stream: true                  # Stream responses
      
      # Model alternatives for different use cases
      alternative_models:
        fast: "llama3.2:3b"         # Faster responses
        quality: "llama3.1:8b"      # Higher quality
        multilingual: "qwen2.5:14b" # Better multilingual
      
      # Performance settings
      num_ctx: 4096                 # Context window
      num_predict: 2000            # Max prediction tokens
      repeat_penalty: 1.1          # Repetition penalty
      
    openai:
      model: "gpt-4o-mini"          # Cost-effective model
      api_key_env: "OPENAI_API_KEY"
      temperature: 0.1
      max_tokens: 2000
      max_retries: 3
      timeout: 30
      
      alternative_models:
        quality: "gpt-4o"           # Higher quality
        speed: "gpt-3.5-turbo"      # Faster responses

# Vector Store Configuration (Existing - ChromaDB)
vectorstore:
  provider: "chroma"
  collection_name: "streamworks_documents"
  persist_directory: "./storage/chroma"
  
  # ChromaDB specific settings
  chroma:
    distance_function: "cosine"     # cosine|euclidean|manhattan
    ef_construction: 200            # HNSW parameter
    ef_search: 100                 # Search parameter
    max_elements: 100000          # Maximum elements

# RAG Pipeline Settings
rag:
  # Retrieval settings
  retrieval:
    top_k: 10                     # Initial retrieval count
    similarity_threshold: 0.1     # Minimum similarity score
    
  # Reranking settings  
  reranking:
    enabled: true
    top_k: 5                      # Final result count after reranking
    score_threshold: 0.3          # Minimum rerank score
    
  # Generation settings
  generation:
    max_context_length: 8000      # Max context for LLM
    include_sources: true         # Include source citations
    system_prompt: |
      Du bist ein hilfreicher AI-Assistent für das Streamworks-KI System.
      Antworte auf Deutsch und nutze die bereitgestellten Quellen für präzise Antworten.
      Zitiere relevante Quellen in deiner Antwort.

# Development & Debug Settings
debug:
  log_level: "INFO"               # DEBUG|INFO|WARNING|ERROR
  log_retrieval_results: false   # Log retrieval details
  log_rerank_scores: false       # Log reranking scores
  log_llm_prompts: false         # Log LLM prompts (sensitive)
  performance_monitoring: true    # Monitor response times
  
# Cache Settings (Optional - for future implementation)
cache:
  enabled: false                  # Enable caching
  provider: "redis"               # redis|memory
  ttl_seconds: 3600              # Cache TTL
  redis_url: "redis://localhost:6379/0"

# Health Check Settings
health_check:
  enabled: true
  interval_seconds: 300          # Check every 5 minutes
  timeout_seconds: 10            # Health check timeout
  retry_attempts: 3              # Retries on failure