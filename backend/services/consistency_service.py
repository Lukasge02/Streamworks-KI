"""
Consistency Service
Ensures data consistency between database and Qdrant vector store
"""

import asyncio
import logging
from typing import Dict, List, Set, Any, Optional, Tuple
from datetime import datetime
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from models.core import Document

logger = logging.getLogger(__name__)


class ConsistencyIssue:
    """Represents a consistency issue found in the system"""

    def __init__(
        self,
        issue_type: str,
        severity: str,
        description: str,
        entity_id: str,
        metadata: Optional[Dict[str, Any]] = None
    ):
        self.issue_type = issue_type
        self.severity = severity  # 'critical', 'warning', 'info'
        self.description = description
        self.entity_id = entity_id
        self.metadata = metadata or {}
        self.detected_at = datetime.now().isoformat()


class ConsistencyService:
    """
    Service for monitoring and repairing data consistency between DB and Qdrant
    """

    def __init__(self):
        self.issues: List[ConsistencyIssue] = []

    async def perform_full_consistency_check(self, db: AsyncSession) -> Dict[str, Any]:
        """
        Comprehensive consistency check between database and Qdrant

        Returns:
            Dictionary with detailed consistency report
        """
        logger.info("ðŸ” Starting comprehensive consistency check...")

        self.issues = []  # Reset issues list
        start_time = datetime.now()

        try:
            # Step 1: Get all documents from database
            db_documents = await self._get_all_db_documents(db)
            db_doc_ids = {str(doc.id) for doc in db_documents}

            logger.info(f"ðŸ“Š Found {len(db_documents)} documents in database")

            # Step 2: Get all document IDs from Qdrant
            qdrant_doc_ids = await self._get_all_qdrant_document_ids()

            logger.info(f"ðŸ“Š Found {len(qdrant_doc_ids)} unique document IDs in Qdrant")

            # Step 3: Find inconsistencies
            orphaned_chunks = qdrant_doc_ids - db_doc_ids  # In Qdrant but not in DB
            missing_chunks = db_doc_ids - qdrant_doc_ids   # In DB but not in Qdrant

            # Record orphaned chunks (critical issue)
            for doc_id in orphaned_chunks:
                chunk_count = await self._count_qdrant_chunks_for_doc(doc_id)
                self.issues.append(ConsistencyIssue(
                    issue_type="orphaned_chunks",
                    severity="critical",
                    description=f"Document {doc_id} has {chunk_count} chunks in Qdrant but no database record",
                    entity_id=doc_id,
                    metadata={"chunk_count": chunk_count}
                ))

            # Record missing chunks (warning - could be processing)
            for doc_id in missing_chunks:
                # Check if document is still processing
                doc = next((d for d in db_documents if str(d.id) == doc_id), None)
                if doc:
                    if hasattr(doc, 'status') and doc.status in ['processing', 'pending']:
                        severity = "info"
                        description = f"Document {doc_id} ({doc.original_filename}) is still processing - chunks not yet created"
                    else:
                        severity = "warning"
                        description = f"Document {doc_id} ({doc.original_filename}) has no chunks in Qdrant but is marked as ready"

                    self.issues.append(ConsistencyIssue(
                        issue_type="missing_chunks",
                        severity=severity,
                        description=description,
                        entity_id=doc_id,
                        metadata={"filename": doc.original_filename, "status": getattr(doc, 'status', 'unknown')}
                    ))

            # Step 4: Check chunk integrity for synchronized documents
            synchronized_docs = db_doc_ids & qdrant_doc_ids
            for doc_id in synchronized_docs:
                doc = next((d for d in db_documents if str(d.id) == doc_id), None)
                if doc:
                    await self._check_document_chunk_integrity(doc_id, doc)

            # Generate report
            processing_time = (datetime.now() - start_time).total_seconds()

            report = {
                "timestamp": start_time.isoformat(),
                "processing_time_seconds": round(processing_time, 2),
                "summary": {
                    "total_documents_db": len(db_documents),
                    "total_document_ids_qdrant": len(qdrant_doc_ids),
                    "synchronized_documents": len(synchronized_docs),
                    "orphaned_chunks_count": len(orphaned_chunks),
                    "missing_chunks_count": len(missing_chunks),
                    "total_issues": len(self.issues)
                },
                "issues_by_severity": {
                    "critical": len([i for i in self.issues if i.severity == "critical"]),
                    "warning": len([i for i in self.issues if i.severity == "warning"]),
                    "info": len([i for i in self.issues if i.severity == "info"])
                },
                "detailed_issues": [
                    {
                        "type": issue.issue_type,
                        "severity": issue.severity,
                        "description": issue.description,
                        "entity_id": issue.entity_id,
                        "metadata": issue.metadata,
                        "detected_at": issue.detected_at
                    }
                    for issue in self.issues
                ],
                "consistency_status": self._calculate_consistency_status(),
                "recommendations": self._generate_recommendations()
            }

            logger.info(f"âœ… Consistency check completed: {len(self.issues)} issues found")
            return report

        except Exception as e:
            logger.error(f"âŒ Consistency check failed: {str(e)}")
            return {
                "timestamp": start_time.isoformat(),
                "error": str(e),
                "summary": {"total_issues": 0},
                "consistency_status": "error"
            }

    async def _get_all_db_documents(self, db: AsyncSession) -> List[Document]:
        """Get all documents from database"""
        try:
            result = await db.execute(select(Document))
            return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to fetch documents from database: {str(e)}")
            return []

    async def _get_all_qdrant_document_ids(self) -> Set[str]:
        """Get all unique document IDs from Qdrant"""
        try:
            from services.qdrant_rag_service import get_rag_service
            rag_service = await get_rag_service()

            if not await rag_service.initialize():
                logger.error("Failed to initialize Qdrant service")
                return set()

            # Use scroll to get all points with doc_id metadata
            doc_ids = set()

            # This is a simplified approach - in production, we'd need to scroll through all points
            # For now, we'll use the collection info to get a count
            collection_info = await rag_service.qdrant_service.get_collection_info()
            points_count = collection_info.get("points_count", 0)

            if points_count > 0:
                # We can't easily get all doc_ids without a full scan
                # For this demo, we'll indicate that there are points but we can't list them efficiently
                logger.info(f"Qdrant has {points_count} points - detailed doc_id extraction requires full scan")

                # Alternative: Try to get some sample doc_ids from recent documents
                # This is not comprehensive but gives us an idea of what's there
                try:
                    import asyncio
                    from qdrant_client.models import Filter, FieldCondition, MatchAny

                    # Try to scroll through some points to get doc_ids
                    client = rag_service.qdrant_service.client
                    collection_name = rag_service.qdrant_service.collection_name

                    scroll_result = await asyncio.to_thread(
                        client.scroll,
                        collection_name=collection_name,
                        limit=min(100, points_count),  # Sample first 100 points
                        with_payload=True,
                        with_vectors=False
                    )

                    points = scroll_result[0] if scroll_result else []
                    for point in points:
                        if hasattr(point, 'payload') and 'doc_id' in point.payload:
                            doc_ids.add(point.payload['doc_id'])

                    logger.info(f"Sampled {len(doc_ids)} unique document IDs from {len(points)} Qdrant points")

                except Exception as sample_error:
                    logger.warning(f"Could not sample Qdrant doc_ids: {str(sample_error)}")

            return doc_ids

        except Exception as e:
            logger.error(f"Failed to get document IDs from Qdrant: {str(e)}")
            return set()

    async def _count_qdrant_chunks_for_doc(self, doc_id: str) -> int:
        """Count chunks for a specific document in Qdrant"""
        try:
            from services.qdrant_rag_service import get_rag_service
            rag_service = await get_rag_service()

            chunks = await rag_service.qdrant_service.get_document_chunks(doc_id, limit=1000)
            return len(chunks)

        except Exception as e:
            logger.warning(f"Could not count chunks for {doc_id}: {str(e)}")
            return 0

    async def _check_document_chunk_integrity(self, doc_id: str, document: Document) -> None:
        """Check chunk integrity for a synchronized document"""
        try:
            chunk_count = await self._count_qdrant_chunks_for_doc(doc_id)
            expected_chunks = getattr(document, 'chunk_count', 0)

            if chunk_count != expected_chunks:
                self.issues.append(ConsistencyIssue(
                    issue_type="chunk_count_mismatch",
                    severity="warning",
                    description=f"Document {doc_id} has {chunk_count} chunks in Qdrant but {expected_chunks} expected",
                    entity_id=doc_id,
                    metadata={
                        "filename": document.original_filename,
                        "actual_chunks": chunk_count,
                        "expected_chunks": expected_chunks
                    }
                ))

        except Exception as e:
            logger.warning(f"Could not check chunk integrity for {doc_id}: {str(e)}")

    def _calculate_consistency_status(self) -> str:
        """Calculate overall consistency status"""
        if not self.issues:
            return "âœ… consistent"

        critical_count = len([i for i in self.issues if i.severity == "critical"])
        warning_count = len([i for i in self.issues if i.severity == "warning"])

        if critical_count > 0:
            return f"âŒ critical_issues ({critical_count} critical, {warning_count} warnings)"
        elif warning_count > 0:
            return f"âš ï¸ warnings ({warning_count} warnings)"
        else:
            return "â„¹ï¸ minor_issues"

    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on found issues"""
        recommendations = []

        orphaned_count = len([i for i in self.issues if i.issue_type == "orphaned_chunks"])
        missing_count = len([i for i in self.issues if i.issue_type == "missing_chunks"])

        if orphaned_count > 0:
            recommendations.append(f"ðŸ—‘ï¸ Clean up {orphaned_count} orphaned chunk groups from Qdrant")

        if missing_count > 0:
            recommendations.append(f"ðŸ”„ Reprocess {missing_count} documents to generate missing chunks")

        if len(self.issues) > 5:
            recommendations.append("ðŸ”§ Consider running automated repair service")

        return recommendations

    async def cleanup_orphaned_chunks(self, dry_run: bool = True) -> Dict[str, Any]:
        """
        Clean up orphaned chunks from Qdrant (chunks without corresponding DB documents)

        Args:
            dry_run: If True, only simulate the cleanup

        Returns:
            Cleanup report
        """
        logger.info(f"ðŸ—‘ï¸ Starting orphaned chunk cleanup (dry_run={dry_run})")

        orphaned_issues = [i for i in self.issues if i.issue_type == "orphaned_chunks"]

        if not orphaned_issues:
            return {
                "action": "cleanup_orphaned_chunks",
                "dry_run": dry_run,
                "orphaned_chunks_found": 0,
                "cleanup_results": [],
                "message": "No orphaned chunks found"
            }

        cleanup_results = []

        for issue in orphaned_issues:
            doc_id = issue.entity_id

            if dry_run:
                cleanup_results.append({
                    "doc_id": doc_id,
                    "action": "would_delete",
                    "chunk_count": issue.metadata.get("chunk_count", 0)
                })
            else:
                try:
                    from services.qdrant_rag_service import get_rag_service
                    rag_service = await get_rag_service()
                    success = await rag_service.delete_document(doc_id)

                    cleanup_results.append({
                        "doc_id": doc_id,
                        "action": "deleted" if success else "failed",
                        "chunk_count": issue.metadata.get("chunk_count", 0),
                        "success": success
                    })

                except Exception as e:
                    cleanup_results.append({
                        "doc_id": doc_id,
                        "action": "error",
                        "error": str(e)
                    })

        return {
            "action": "cleanup_orphaned_chunks",
            "dry_run": dry_run,
            "orphaned_chunks_found": len(orphaned_issues),
            "cleanup_results": cleanup_results,
            "message": f"Processed {len(orphaned_issues)} orphaned chunk groups"
        }


# Global service instance
_consistency_service = None

async def get_consistency_service() -> ConsistencyService:
    """Get global consistency service instance"""
    global _consistency_service
    if _consistency_service is None:
        _consistency_service = ConsistencyService()
    return _consistency_service