"""
Result Fusion Service - Advanced Multi-Query Result Integration
Intelligently combines and deduplicates results from multiple query variants
"""

import asyncio
import logging
import time
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from collections import defaultdict
import re
from difflib import SequenceMatcher

from .hybrid_retriever import HybridSearchResult
from .query_expander import ExpandedQuery

logger = logging.getLogger(__name__)

@dataclass
class FusedResult:
    """Represents a result after multi-query fusion"""
    chunk_id: str
    doc_id: str
    content: str
    metadata: Dict[str, Any]
    
    # Aggregated scoring
    final_score: float
    confidence: float
    
    # Source query information
    source_queries: List[str] = field(default_factory=list)
    query_scores: Dict[str, float] = field(default_factory=dict)
    query_ranks: Dict[str, int] = field(default_factory=dict)
    
    # Fusion metadata
    fusion_method: str = \"weighted_sum\"\n    diversity_score: float = 0.0\n    consistency_score: float = 0.0\n    coverage_score: float = 0.0\n    \n    # Quality indicators\n    matched_queries: int = 0\n    avg_rank: float = 0.0\n    rank_variance: float = 0.0\n    \nclass FusionStrategy:\n    \"\"\"Different fusion strategies\"\"\"\n    WEIGHTED_SUM = \"weighted_sum\"          # Sum weighted scores\n    MAX_SCORE = \"max_score\"                # Take maximum score\n    RANK_FUSION = \"rank_fusion\"            # Reciprocal rank fusion\n    BAYESIAN = \"bayesian\"                  # Bayesian combination\n    LEARNED_WEIGHTS = \"learned_weights\"    # ML-based weights\n\nclass ResultFusion:\n    \"\"\"\n    Advanced Multi-Query Result Fusion System\n    \n    Features:\n    - Multiple fusion strategies (RRF, weighted sum, Bayesian)\n    - Intelligent deduplication with semantic similarity\n    - Quality-based result filtering and ranking\n    - Diversity optimization to avoid redundant results\n    - Performance monitoring and adaptive tuning\n    \"\"\"\n    \n    def __init__(self,\n                 similarity_threshold: float = 0.85,\n                 max_results: int = 10,\n                 diversity_weight: float = 0.1):\n        \"\"\"\n        Initialize Result Fusion service\n        \n        Args:\n            similarity_threshold: Threshold for duplicate detection\n            max_results: Maximum number of results to return\n            diversity_weight: Weight for diversity in final ranking\n        \"\"\"\n        self.similarity_threshold = similarity_threshold\n        self.max_results = max_results\n        self.diversity_weight = diversity_weight\n        \n        # Performance tracking\n        self.fusion_stats = {\n            'total_fusions': 0,\n            'total_input_results': 0,\n            'total_output_results': 0,\n            'avg_fusion_time': 0.0,\n            'deduplication_rate': 0.0\n        }\n        \n        logger.info(f\"ðŸ”€ ResultFusion initialized - similarity: {similarity_threshold}, diversity: {diversity_weight}\")\n    \n    async def fuse_multi_query_results(\n        self,\n        query_results: Dict[str, List[HybridSearchResult]],\n        original_query: str,\n        fusion_strategy: str = FusionStrategy.RANK_FUSION,\n        query_weights: Optional[Dict[str, float]] = None\n    ) -> List[FusedResult]:\n        \"\"\"\n        Fuse results from multiple query variants\n        \n        Args:\n            query_results: Dictionary mapping query â†’ results\n            original_query: Original user query for weighting\n            fusion_strategy: Fusion strategy to use\n            query_weights: Optional weights for each query\n            \n        Returns:\n            List of fused and ranked results\n        \"\"\"\n        start_time = time.time()\n        self.fusion_stats['total_fusions'] += 1\n        \n        try:\n            if not query_results:\n                return []\n            \n            # Count input results\n            total_input = sum(len(results) for results in query_results.values())\n            self.fusion_stats['total_input_results'] += total_input\n            \n            # Step 1: Deduplicate within each query's results\n            deduplicated_results = {}\n            for query, results in query_results.items():\n                deduplicated_results[query] = await self._deduplicate_results(results)\n            \n            # Step 2: Build candidate pool with cross-query deduplication\n            candidate_pool = await self._build_candidate_pool(deduplicated_results)\n            \n            # Step 3: Apply fusion strategy\n            if fusion_strategy == FusionStrategy.RANK_FUSION:\n                fused_results = self._reciprocal_rank_fusion(\n                    candidate_pool, deduplicated_results, query_weights\n                )\n            elif fusion_strategy == FusionStrategy.WEIGHTED_SUM:\n                fused_results = self._weighted_sum_fusion(\n                    candidate_pool, deduplicated_results, query_weights\n                )\n            elif fusion_strategy == FusionStrategy.MAX_SCORE:\n                fused_results = self._max_score_fusion(\n                    candidate_pool, deduplicated_results\n                )\n            else:\n                fused_results = self._reciprocal_rank_fusion(\n                    candidate_pool, deduplicated_results, query_weights\n                )\n            \n            # Step 4: Optimize for diversity\n            diverse_results = self._optimize_diversity(fused_results, original_query)\n            \n            # Step 5: Final filtering and ranking\n            final_results = self._final_ranking(diverse_results)\n            \n            # Update statistics\n            fusion_time = time.time() - start_time\n            self.fusion_stats['total_output_results'] += len(final_results)\n            self._update_fusion_stats(fusion_time, total_input, len(final_results))\n            \n            logger.info(f\"Multi-query fusion: {len(query_results)} queries, {total_input} results â†’ {len(final_results)} fused ({fusion_time:.3f}s)\")\n            return final_results\n            \n        except Exception as e:\n            logger.error(f\"Multi-query fusion failed: {str(e)}\")\n            return []\n    \n    async def _deduplicate_results(self, results: List[HybridSearchResult]) -> List[HybridSearchResult]:\n        \"\"\"\n        Remove duplicates within a single result set\n        \n        Uses both exact chunk_id matching and semantic similarity\n        \"\"\"\n        if len(results) <= 1:\n            return results\n        \n        # First pass: exact ID matching\n        seen_ids = set()\n        id_deduplicated = []\n        \n        for result in results:\n            if result.chunk_id not in seen_ids:\n                seen_ids.add(result.chunk_id)\n                id_deduplicated.append(result)\n        \n        # Second pass: semantic similarity for remaining results\n        if len(id_deduplicated) <= 1:\n            return id_deduplicated\n        \n        final_results = []\n        \n        for candidate in id_deduplicated:\n            is_duplicate = False\n            \n            for existing in final_results:\n                similarity = self._calculate_content_similarity(\n                    candidate.content, existing.content\n                )\n                \n                if similarity >= self.similarity_threshold:\n                    is_duplicate = True\n                    # Keep the higher-scoring result\n                    if candidate.final_score > existing.final_score:\n                        final_results.remove(existing)\n                        final_results.append(candidate)\n                    break\n            \n            if not is_duplicate:\n                final_results.append(candidate)\n        \n        return final_results\n    \n    async def _build_candidate_pool(\n        self,\n        query_results: Dict[str, List[HybridSearchResult]]\n    ) -> Dict[str, List[Tuple[str, HybridSearchResult]]]:\n        \"\"\"\n        Build unified candidate pool with cross-query deduplication\n        \n        Returns:\n            Dictionary mapping chunk_id â†’ [(query, result), ...]\n        \"\"\"\n        candidate_pool = defaultdict(list)\n        \n        # Group results by chunk_id\n        for query, results in query_results.items():\n            for result in results:\n                candidate_pool[result.chunk_id].append((query, result))\n        \n        # Handle semantic duplicates across different chunk_ids\n        merged_pool = {}\n        processed_chunks = set()\n        \n        for chunk_id, query_result_pairs in candidate_pool.items():\n            if chunk_id in processed_chunks:\n                continue\n            \n            # Find semantically similar chunks\n            similar_chunks = [chunk_id]\n            primary_content = query_result_pairs[0][1].content\n            \n            for other_chunk_id, other_pairs in candidate_pool.items():\n                if other_chunk_id == chunk_id or other_chunk_id in processed_chunks:\n                    continue\n                \n                other_content = other_pairs[0][1].content\n                similarity = self._calculate_content_similarity(primary_content, other_content)\n                \n                if similarity >= self.similarity_threshold:\n                    similar_chunks.append(other_chunk_id)\n                    processed_chunks.add(other_chunk_id)\n            \n            # Merge similar chunks\n            merged_pairs = []\n            for similar_chunk in similar_chunks:\n                merged_pairs.extend(candidate_pool[similar_chunk])\n                processed_chunks.add(similar_chunk)\n            \n            # Use the chunk_id with the highest overall score\n            best_chunk_id = max(\n                similar_chunks,\n                key=lambda cid: max(pair[1].final_score for pair in candidate_pool[cid])\n            )\n            \n            merged_pool[best_chunk_id] = merged_pairs\n        \n        return merged_pool\n    \n    def _reciprocal_rank_fusion(\n        self,\n        candidate_pool: Dict[str, List[Tuple[str, HybridSearchResult]]],\n        query_results: Dict[str, List[HybridSearchResult]],\n        query_weights: Optional[Dict[str, float]] = None,\n        k: int = 60\n    ) -> List[FusedResult]:\n        \"\"\"\n        Apply Reciprocal Rank Fusion (RRF) across multiple queries\n        \n        Args:\n            candidate_pool: Merged candidate results\n            query_results: Original query results for ranking\n            query_weights: Optional query importance weights\n            k: RRF parameter (typically 60)\n            \n        Returns:\n            List of fused results with RRF scores\n        \"\"\"\n        if not query_weights:\n            query_weights = {query: 1.0 for query in query_results.keys()}\n        \n        # Normalize weights\n        total_weight = sum(query_weights.values())\n        if total_weight > 0:\n            query_weights = {q: w/total_weight for q, w in query_weights.items()}\n        \n        # Build rank lookup for each query\n        query_rankings = {}\n        for query, results in query_results.items():\n            query_rankings[query] = {r.chunk_id: i+1 for i, r in enumerate(results)}\n        \n        fused_results = []\n        \n        for chunk_id, query_result_pairs in candidate_pool.items():\n            # Calculate RRF score\n            rrf_score = 0.0\n            source_queries = []\n            query_scores = {}\n            query_ranks = {}\n            \n            # Get best result representation (highest scoring)\n            best_result = max(query_result_pairs, key=lambda x: x[1].final_score)[1]\n            \n            for query, result in query_result_pairs:\n                rank = query_rankings[query].get(chunk_id, len(query_results[query]) + 1)\n                weight = query_weights.get(query, 1.0)\n                \n                # RRF contribution\n                rrf_contribution = weight / (k + rank)\n                rrf_score += rrf_contribution\n                \n                source_queries.append(query)\n                query_scores[query] = result.final_score\n                query_ranks[query] = rank\n            \n            # Calculate additional quality metrics\n            ranks = list(query_ranks.values())\n            avg_rank = sum(ranks) / len(ranks) if ranks else 0.0\n            rank_variance = sum((r - avg_rank) ** 2 for r in ranks) / len(ranks) if len(ranks) > 1 else 0.0\n            \n            # Calculate consistency score (lower variance = higher consistency)\n            consistency_score = 1.0 / (1.0 + rank_variance)\n            \n            # Create fused result\n            fused_result = FusedResult(\n                chunk_id=chunk_id,\n                doc_id=best_result.doc_id,\n                content=best_result.content,\n                metadata=best_result.metadata,\n                final_score=rrf_score,\n                confidence=consistency_score,\n                source_queries=source_queries,\n                query_scores=query_scores,\n                query_ranks=query_ranks,\n                fusion_method=FusionStrategy.RANK_FUSION,\n                consistency_score=consistency_score,\n                matched_queries=len(source_queries),\n                avg_rank=avg_rank,\n                rank_variance=rank_variance\n            )\n            \n            fused_results.append(fused_result)\n        \n        # Sort by RRF score\n        fused_results.sort(key=lambda x: x.final_score, reverse=True)\n        return fused_results\n    \n    def _weighted_sum_fusion(\n        self,\n        candidate_pool: Dict[str, List[Tuple[str, HybridSearchResult]]],\n        query_results: Dict[str, List[HybridSearchResult]],\n        query_weights: Optional[Dict[str, float]] = None\n    ) -> List[FusedResult]:\n        \"\"\"\n        Apply weighted sum fusion across multiple queries\n        \"\"\"\n        if not query_weights:\n            query_weights = {query: 1.0 for query in query_results.keys()}\n        \n        # Normalize weights\n        total_weight = sum(query_weights.values())\n        if total_weight > 0:\n            query_weights = {q: w/total_weight for q, w in query_weights.items()}\n        \n        fused_results = []\n        \n        for chunk_id, query_result_pairs in candidate_pool.items():\n            # Calculate weighted sum score\n            weighted_score = 0.0\n            source_queries = []\n            query_scores = {}\n            \n            # Get best result representation\n            best_result = max(query_result_pairs, key=lambda x: x[1].final_score)[1]\n            \n            for query, result in query_result_pairs:\n                weight = query_weights.get(query, 1.0)\n                contribution = weight * result.final_score\n                weighted_score += contribution\n                \n                source_queries.append(query)\n                query_scores[query] = result.final_score\n            \n            # Calculate confidence based on score consistency\n            scores = list(query_scores.values())\n            if len(scores) > 1:\n                score_mean = sum(scores) / len(scores)\n                score_variance = sum((s - score_mean) ** 2 for s in scores) / len(scores)\n                confidence = 1.0 / (1.0 + score_variance)\n            else:\n                confidence = 0.8\n            \n            # Create fused result\n            fused_result = FusedResult(\n                chunk_id=chunk_id,\n                doc_id=best_result.doc_id,\n                content=best_result.content,\n                metadata=best_result.metadata,\n                final_score=weighted_score,\n                confidence=confidence,\n                source_queries=source_queries,\n                query_scores=query_scores,\n                fusion_method=FusionStrategy.WEIGHTED_SUM,\n                matched_queries=len(source_queries)\n            )\n            \n            fused_results.append(fused_result)\n        \n        # Sort by weighted score\n        fused_results.sort(key=lambda x: x.final_score, reverse=True)\n        return fused_results\n    \n    def _max_score_fusion(\n        self,\n        candidate_pool: Dict[str, List[Tuple[str, HybridSearchResult]]],\n        query_results: Dict[str, List[HybridSearchResult]]\n    ) -> List[FusedResult]:\n        \"\"\"\n        Apply max score fusion - take the highest score for each result\n        \"\"\"\n        fused_results = []\n        \n        for chunk_id, query_result_pairs in candidate_pool.items():\n            # Find the highest scoring result\n            best_query, best_result = max(query_result_pairs, key=lambda x: x[1].final_score)\n            \n            # Collect all source information\n            source_queries = [query for query, _ in query_result_pairs]\n            query_scores = {query: result.final_score for query, result in query_result_pairs}\n            \n            # Create fused result\n            fused_result = FusedResult(\n                chunk_id=chunk_id,\n                doc_id=best_result.doc_id,\n                content=best_result.content,\n                metadata=best_result.metadata,\n                final_score=best_result.final_score,\n                confidence=0.7,  # Moderate confidence for max score\n                source_queries=source_queries,\n                query_scores=query_scores,\n                fusion_method=FusionStrategy.MAX_SCORE,\n                matched_queries=len(source_queries)\n            )\n            \n            fused_results.append(fused_result)\n        \n        # Sort by max score\n        fused_results.sort(key=lambda x: x.final_score, reverse=True)\n        return fused_results\n    \n    def _optimize_diversity(\n        self,\n        fused_results: List[FusedResult],\n        original_query: str\n    ) -> List[FusedResult]:\n        \"\"\"\n        Optimize result set for diversity to avoid redundant information\n        \n        Uses Maximal Marginal Relevance (MMR) approach\n        \"\"\"\n        if len(fused_results) <= 1:\n            return fused_results\n        \n        # MMR selection\n        selected_results = []\n        remaining_results = fused_results.copy()\n        \n        # Select the highest scoring result first\n        if remaining_results:\n            best_result = remaining_results.pop(0)\n            selected_results.append(best_result)\n        \n        # Select remaining results using MMR\n        while remaining_results and len(selected_results) < self.max_results:\n            mmr_scores = []\n            \n            for candidate in remaining_results:\n                # Relevance score (already computed)\n                relevance = candidate.final_score\n                \n                # Calculate similarity to already selected results\n                max_similarity = 0.0\n                for selected in selected_results:\n                    similarity = self._calculate_content_similarity(\n                        candidate.content, selected.content\n                    )\n                    max_similarity = max(max_similarity, similarity)\n                \n                # MMR score: relevance - Î» * max_similarity\n                mmr_score = relevance - self.diversity_weight * max_similarity\n                mmr_scores.append((mmr_score, candidate))\n            \n            # Select candidate with highest MMR score\n            if mmr_scores:\n                best_mmr_score, best_candidate = max(mmr_scores, key=lambda x: x[0])\n                selected_results.append(best_candidate)\n                remaining_results.remove(best_candidate)\n                \n                # Update diversity score for the selected candidate\n                best_candidate.diversity_score = best_mmr_score\n        \n        return selected_results\n    \n    def _final_ranking(\n        self,\n        diverse_results: List[FusedResult]\n    ) -> List[FusedResult]:\n        \"\"\"\n        Apply final ranking considering multiple quality factors\n        \"\"\"\n        def calculate_final_ranking_score(result: FusedResult) -> float:\n            # Base score from fusion\n            base_score = result.final_score\n            \n            # Boost for consistency across queries\n            consistency_boost = result.consistency_score * 0.1\n            \n            # Boost for multiple query matches\n            coverage_boost = min(result.matched_queries / 3.0, 1.0) * 0.1\n            \n            # Penalize very high ranks (lower quality)\n            rank_penalty = max(0, (result.avg_rank - 5) * 0.01) if result.avg_rank > 0 else 0\n            \n            # Diversity bonus\n            diversity_bonus = result.diversity_score * 0.05\n            \n            final_score = (\n                base_score + \n                consistency_boost + \n                coverage_boost + \n                diversity_bonus - \n                rank_penalty\n            )\n            \n            return final_score\n        \n        # Calculate final ranking scores\n        for result in diverse_results:\n            result.coverage_score = min(result.matched_queries / 3.0, 1.0)\n            final_ranking_score = calculate_final_ranking_score(result)\n            result.final_score = final_ranking_score\n        \n        # Sort by final ranking score\n        diverse_results.sort(key=lambda x: x.final_score, reverse=True)\n        \n        # Return top results\n        return diverse_results[:self.max_results]\n    \n    def _calculate_content_similarity(self, content1: str, content2: str) -> float:\n        \"\"\"\n        Calculate semantic similarity between two content strings\n        \n        Uses multiple similarity measures for robustness\n        \"\"\"\n        if not content1 or not content2:\n            return 0.0\n        \n        # Normalize content\n        content1_clean = self._normalize_text(content1)\n        content2_clean = self._normalize_text(content2)\n        \n        if content1_clean == content2_clean:\n            return 1.0\n        \n        # Multiple similarity measures\n        similarities = []\n        \n        # 1. Sequence matcher (character-level)\n        seq_similarity = SequenceMatcher(None, content1_clean, content2_clean).ratio()\n        similarities.append(seq_similarity)\n        \n        # 2. Jaccard similarity (word-level)\n        words1 = set(content1_clean.split())\n        words2 = set(content2_clean.split())\n        if words1 or words2:\n            jaccard_similarity = len(words1 & words2) / len(words1 | words2)\n            similarities.append(jaccard_similarity)\n        \n        # 3. Length-normalized edit distance\n        try:\n            max_len = max(len(content1_clean), len(content2_clean))\n            if max_len > 0:\n                edit_distance = self._levenshtein_distance(content1_clean, content2_clean)\n                edit_similarity = 1.0 - (edit_distance / max_len)\n                similarities.append(max(0.0, edit_similarity))\n        except:\n            pass  # Skip if computation fails\n        \n        # Return weighted average\n        if similarities:\n            return sum(similarities) / len(similarities)\n        else:\n            return 0.0\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normalize text for similarity comparison\"\"\"\n        # Convert to lowercase\n        text = text.lower()\n        \n        # Remove extra whitespace\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        # Remove common punctuation\n        text = re.sub(r'[.,;:!?\\-\"\\']', '', text)\n        \n        return text\n    \n    def _levenshtein_distance(self, s1: str, s2: str) -> int:\n        \"\"\"Calculate Levenshtein distance between two strings\"\"\"\n        if len(s1) < len(s2):\n            return self._levenshtein_distance(s2, s1)\n        \n        if len(s2) == 0:\n            return len(s1)\n        \n        previous_row = list(range(len(s2) + 1))\n        for i, c1 in enumerate(s1):\n            current_row = [i + 1]\n            for j, c2 in enumerate(s2):\n                insertions = previous_row[j + 1] + 1\n                deletions = current_row[j] + 1\n                substitutions = previous_row[j] + (c1 != c2)\n                current_row.append(min(insertions, deletions, substitutions))\n            previous_row = current_row\n        \n        return previous_row[-1]\n    \n    def _update_fusion_stats(self, fusion_time: float, input_count: int, output_count: int):\n        \"\"\"Update fusion performance statistics\"\"\"\n        total_fusions = self.fusion_stats['total_fusions']\n        old_avg_time = self.fusion_stats['avg_fusion_time']\n        \n        # Update running averages\n        self.fusion_stats['avg_fusion_time'] = (\n            (old_avg_time * (total_fusions - 1) + fusion_time) / total_fusions\n        )\n        \n        # Update deduplication rate\n        if input_count > 0:\n            current_dedup_rate = 1.0 - (output_count / input_count)\n            old_dedup_rate = self.fusion_stats['deduplication_rate']\n            self.fusion_stats['deduplication_rate'] = (\n                (old_dedup_rate * (total_fusions - 1) + current_dedup_rate) / total_fusions\n            )\n    \n    def get_fusion_stats(self) -> Dict[str, Any]:\n        \"\"\"Get fusion performance statistics\"\"\"\n        return {\n            **self.fusion_stats,\n            'parameters': {\n                'similarity_threshold': self.similarity_threshold,\n                'max_results': self.max_results,\n                'diversity_weight': self.diversity_weight\n            },\n            'avg_input_per_fusion': (\n                self.fusion_stats['total_input_results'] / \n                max(1, self.fusion_stats['total_fusions'])\n            ),\n            'avg_output_per_fusion': (\n                self.fusion_stats['total_output_results'] / \n                max(1, self.fusion_stats['total_fusions'])\n            )\n        }\n    \n    def adjust_parameters(self,\n                         similarity_threshold: Optional[float] = None,\n                         diversity_weight: Optional[float] = None,\n                         max_results: Optional[int] = None):\n        \"\"\"Dynamically adjust fusion parameters\"\"\"\n        if similarity_threshold is not None:\n            self.similarity_threshold = similarity_threshold\n        if diversity_weight is not None:\n            self.diversity_weight = diversity_weight\n        if max_results is not None:\n            self.max_results = max_results\n            \n        logger.info(\n            f\"Fusion parameters updated: sim={self.similarity_threshold}, \"\n            f\"div={self.diversity_weight}, max={self.max_results}\"\n        )